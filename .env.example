# LLM Inference API Configuration
# Copy this file to .env and customize as needed

# Timeout configurations (in seconds)
CHAT_TIMEOUT_SECONDS=90
MODEL_PULL_TIMEOUT_SECONDS=3600

# API URLs
OLLAMA_BASE_URL=http://ollama:11434
BACKEND_URL=http://backend:8000

# Logging
LOG_LEVEL=INFO

# Environment
ENVIRONMENT=development
